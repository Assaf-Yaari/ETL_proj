{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "path_arr = ['stage','stage/log','stage/raw_converted','stage/extra_data','stage/transformed_db']\n",
    "for stag_path in path_arr:\n",
    "    if not os.path.exists(stag_path):\n",
    "        os.makedirs(stag_path)\n",
    "\n",
    "log_path = 'stage/log/log_{}.txt'.format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "def log_managment(msg):\n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(msg)\n",
    "\n",
    "log_managment(('Pipeline intiated{}\\n').format(log_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the code will be exported from the json files acting as a source into a stagin area and saved into csv files to allow make them easier to handle\n",
    "\n",
    "Libraries used: \n",
    "\n",
    "os - used to scan through local folder for source files\n",
    "\n",
    "pandas - used to crate backup of source files in csv format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is list\n",
    "# inside data is a dict \n",
    "try:\n",
    "    dbdict = {}\n",
    "    sourcedf = {}\n",
    "    \n",
    "# load files from source into dictionary \n",
    "    for filename in os.scandir('source'):\n",
    "        if filename.is_file():\n",
    "            dbdict[filename.name] = open(filename.path)\n",
    "\n",
    "    # loop through files to convert into a datafram and backup into\n",
    "    for source_key in dbdict.keys():\n",
    "        data = json.load(dbdict[source_key])\n",
    "        newfile_loc = 'stage/raw_converted/{}.csv'.format(source_key)\n",
    "        sourcedf[source_key] = pd.DataFrame.from_dict(data)  \n",
    "        sourcedf[source_key].to_csv(newfile_loc,index=False,header=True ,sep=',' )\n",
    "        write_msg = ('At {} {} was added to dataframe with {} rows\\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\") \n",
    "            ,source_key,sourcedf[source_key].size )\n",
    "        log_managment(write_msg)\n",
    "        dbdict[source_key].close()\n",
    "except:\n",
    "    write_msg = ('At {} Export has failed \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transfprmation is done to ensure the data is processed into the database in the manner it is described in the task.\n",
    "\n",
    "Coulmn check - Going over columns to check for extra or missing columns\n",
    "\n",
    "Column adjusment - Adding or reducing unnececry columns\n",
    "\n",
    "Formating check - Checking data formats of columns \n",
    "\n",
    "Filling in missing values - Adding values to the columns with missing to allow conversion to right data type.\n",
    "\n",
    "Converting columns to correct type - Converting the columns to the correct type.\n",
    "\n",
    "Backup - Backing up data to a csv in case of a system crash.\n",
    "\n",
    "\n",
    "Libraries used: \n",
    "\n",
    "pandas - used for dataframe managment and to crate backup of source files in csv format \n",
    "\n",
    "numpy - used to create empty array to fill new column and to set data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coulmn check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['address', 'city', 'country', 'cuisines', 'features', 'id_outlet',\n",
      "       'lat', 'lon', 'menu', 'name', 'opening_hours', 'phone', 'postal_code',\n",
      "       'price_level', 'price_range', 'rating', 'region', 'reviews_nr',\n",
      "       'special_diets', 'street', 'tags', 'url', 'website'],\n",
      "      dtype='object')\n",
      "Index(['body', 'date', 'url', 'user', 'rating', 'id_outlet', 'traveler_type'], dtype='object')\n",
      "Index(['user', 'address', 'reviews', 'likes'], dtype='object')\n",
      "Index(['id_outlet', 'name', 'brand', 'price', 'volume'], dtype='object')\n",
      "Index(['id_outlet', 'country', 'name', 'address', 'reviews_nr'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Defining the desired columns names\n",
    "\n",
    "outlet_coulmns = [ 'id_outlet', 'name', 'address', 'country', 'phone', 'reviews_nr']\n",
    "user_coulmns = ['user', 'profile' 'reviews_nr','likes']\n",
    "review_coulmns = ['user', 'id_outlet', 'review', 'rating']\n",
    "menu_columns  = [ 'id_outlet','brand', 'price', 'volume', 'name']\n",
    "\n",
    "for df_keys in sourcedf:\n",
    "    print(sourcedf[df_keys].columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column adjusment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  def coulmn_drop(df_name,column_names):\n",
    "    # Get the difrrence between requierd column names and existing ones \n",
    "    extra_coulmns = set(sourcedf[df_name].columns).difference(column_names)\n",
    "    # set path to save extra data in stage area\n",
    "    csv_filename = 'stage/extra_data/{}.csv'.format(df_name)\n",
    "    # Save extra information in csv file\n",
    "    sourcedf[df_name][extra_coulmns].to_csv(csv_filename,index=False,header=True ,sep=',' )\n",
    "    write_msg = ('At {} {} extra columns were extracted to {} \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"),df_name,csv_filename )\n",
    "    log_managment(write_msg)\n",
    "    # Drop extra columns from dataframe\n",
    "    sourcedf[df_name] = sourcedf[df_name].drop(extra_coulmns , axis =1 )\n",
    "    write_msg = ('At {} {} has dropped extra columns \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"),df_name )\n",
    "    log_managment(write_msg)\n",
    "\n",
    "  # Tripadvisor column name change \n",
    "  sourcedf['tripadvisor_reviews.json'].rename(columns={'body':'review'} , inplace=True)\n",
    "  sourcedf['tripadvisor_user.json'].rename(columns={'reviews':'reviews_nr'}, inplace=True)\n",
    "  # Tripadvisor outlet reducing columns and saving it in a different file\n",
    "  coulmn_drop('tripadvisor_outlet.json',outlet_coulmns)\n",
    "  # Tripadvisor review reducing columns and saving it in a different file\n",
    "  coulmn_drop('tripadvisor_reviews.json',review_coulmns)\n",
    "  #Ubereats outlet add missing coulmns \n",
    "  extra_coulmns = set(outlet_coulmns).difference(sourcedf['ubereats_outlet.json'].columns)\n",
    "  write_msg = ('At {} ubereats_outlet.json column phone added \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\") )\n",
    "  log_managment(write_msg)\n",
    "  sourcedf['ubereats_outlet.json'][list(extra_coulmns)]  = np.nan\n",
    "\n",
    "except:\n",
    "  write_msg = ('At {} Transform Column adjusment has failed \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "  log_managment(write_msg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tripadvisor_outlet.json \n",
      " address        object\n",
      "country        object\n",
      "id_outlet      object\n",
      "name           object\n",
      "phone          object\n",
      "reviews_nr    float64\n",
      "dtype: object\n",
      "tripadvisor_reviews.json \n",
      " review        object\n",
      "user          object\n",
      "rating       float64\n",
      "id_outlet     object\n",
      "dtype: object\n",
      "tripadvisor_user.json \n",
      " user           object\n",
      "address        object\n",
      "reviews_nr      int64\n",
      "likes         float64\n",
      "dtype: object\n",
      "ubereats_menu.json \n",
      " id_outlet     object\n",
      "name          object\n",
      "brand         object\n",
      "price        float64\n",
      "volume        object\n",
      "dtype: object\n",
      "ubereats_outlet.json \n",
      " id_outlet      object\n",
      "country        object\n",
      "name           object\n",
      "address        object\n",
      "reviews_nr      int64\n",
      "phone         float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "for df_keys in sourcedf:\n",
    "    print( df_keys ,\"\\n\",sourcedf[df_keys].dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tripadvisor_outlet\t reviews_nr\n",
      " Missing values:\t False\n",
      "tripadvisor_user\t likes\n",
      " Missing values:\t True\n",
      "ubereats_menu\t volume\n",
      " Missing values:\t False\n"
     ]
    }
   ],
   "source": [
    "# Checking missing entrires in df \n",
    "\n",
    "# tripadvisor_outlet reviews_number\n",
    "print( \"tripadvisor_outlet\\t reviews_nr\\n\", \"Missing values:\\t\", sourcedf['tripadvisor_outlet.json']['reviews_nr'].isnull().values.any())\n",
    "# tripadvisor_user likes\n",
    "print( \"tripadvisor_user\\t likes\\n\", \"Missing values:\\t\", sourcedf['tripadvisor_user.json']['likes'].isnull().values.any())\n",
    "# ubereats_outlet volume\n",
    "print( \"ubereats_menu\\t volume\\n\", \"Missing values:\\t\", sourcedf['ubereats_menu.json']['volume'].isnull().values.any())\n",
    "try:\n",
    "    # Replacing non numeric entries with null\n",
    "    sourcedf['tripadvisor_outlet.json']['reviews_nr']= pd.to_numeric(sourcedf['tripadvisor_outlet.json']['reviews_nr'], errors='coerce') \n",
    "    sourcedf['tripadvisor_user.json']['likes']= pd.to_numeric(sourcedf['tripadvisor_user.json']['likes'], errors='coerce') \n",
    "    sourcedf['ubereats_menu.json']['volume']= pd.to_numeric(sourcedf['ubereats_menu.json']['volume'], errors='coerce') \n",
    "\n",
    "    # Filling empty values\n",
    "    sourcedf['tripadvisor_user.json']['likes'] = sourcedf['tripadvisor_user.json']['likes'].fillna(value =0)\n",
    "    sourcedf['tripadvisor_outlet.json']['reviews_nr']= sourcedf['tripadvisor_outlet.json']['reviews_nr'].fillna(value =0)\n",
    "    sourcedf['ubereats_menu.json']['volume']= sourcedf['ubereats_menu.json']['volume'].fillna(value =0)\n",
    "    \n",
    "    write_msg = ('At {} tripadvisor_outlet.json column reviews_nr edited \\n tripadvisor_user.json  column likes edited \\n ubereats_menu.json column volume edited\\n'\n",
    "    ).format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "except:\n",
    "  write_msg = ('At {} Transform Filling in missing values has failed \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "  log_managment(write_msg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting columns to correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert columns to correct format \n",
    "#Converting tripadvisor_outlet reviews to int64\n",
    "try:\n",
    "    sourcedf['tripadvisor_outlet.json']['reviews_nr'] = sourcedf['tripadvisor_outlet.json']['reviews_nr'].astype(np.int64)\n",
    "    #Converting tripadvisor_user likes to int64\n",
    "    sourcedf['tripadvisor_user.json']['likes'] = sourcedf['tripadvisor_user.json']['likes'].astype(np.int64)\n",
    "    #Converting ubereats_outlet volume to float64\n",
    "    sourcedf['ubereats_menu.json']['volume'] = sourcedf['ubereats_menu.json']['volume'].astype(np.int64)\n",
    "    #Converting ubereats_outlet  phone to object\n",
    "    sourcedf['ubereats_outlet.json']['phone'] = sourcedf['ubereats_outlet.json']['phone'].astype(object)\n",
    "    write_msg = (\"At {} tripadvisor_outlet.json column reviews_nr type change to int64 \\n tripadvisor_user.json column likes type change to int64 \\n ubereats_menu.json column volume type change to int64\\n ubereats_outlet.json column phone type change to object\\n\").format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "\n",
    "except:\n",
    "    write_msg = ('At {} Transform Filling in missing values has failed \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backing up the data in case of a crash in the procedure \n",
    "try:\n",
    "    for key in sourcedf:\n",
    "        csv_filename = 'stage/transformed_db/{}.csv'.format(key)\n",
    "        sourcedf[key].to_csv(csv_filename,index=False,header=True ,sep=',')\n",
    "        write_msg = ('At {} {} saved to {}  \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\") ,key ,csv_filename )\n",
    "        log_managment(write_msg)\n",
    "\n",
    "except:\n",
    "    write_msg = ('At {} Transform Backup has failed \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the tables into a newly created database on MySql \n",
    "\n",
    "Libraries used: \n",
    "\n",
    "mysql.connector - connect to sql library and  run queeries \n",
    "\n",
    "sqlalchemy - used to convert dataframe to SQL table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from sqlalchemy import create_engine, types\n",
    "\n",
    "try:\n",
    "#connect to mysql\n",
    "    db_connector = mysql.connector.connect(host =\"localhost\" , port = \"6603\",user = \"root\",passwd = \"root\")\n",
    "    db_cursor = db_connector.cursor(buffered=True)\n",
    "    # Create tripadvisor database\n",
    "    db_cursor.execute(\"CREATE DATABASE IF NOT EXISTS dashmotecstudy\")\n",
    "    # Create engine to connect to sql database \n",
    "    engine = create_engine('mysql://root:root@localhost:3306/dashmotecstudy') \n",
    "    # Create tables \n",
    "    sourcedf['tripadvisor_outlet.json'].to_sql('tripad_outlet',con=engine,index=False,if_exists='replace')\n",
    "    sourcedf['tripadvisor_user.json'].to_sql('tripad_users',con=engine,index=False,if_exists='replace')\n",
    "    sourcedf['tripadvisor_reviews.json'].to_sql('tripad_reviews',con=engine,index=False,if_exists='replace')\n",
    "    sourcedf['ubereats_menu.json'].to_sql('ubeareats_menu',con=engine,index=False,if_exists='replace')\n",
    "    sourcedf['ubereats_outlet.json'].to_sql('ubeareats_outlet',con=engine,index=False,if_exists='replace')\n",
    "    write_msg = ('At {} All dataframes have been loaded to database \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "\n",
    "except:\n",
    "    write_msg = ('At {} Load creating database has failed \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SQL queeries to modify table entries and to create primay and foreign keys "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tripadvisor table setting relations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set primary keys\n",
    "try:\n",
    "    db_cursor.execute(\"USE dashmotecstudy\")\n",
    "    db_cursor.execute(\"ALTER TABLE tripad_outlet MODIFY id_outlet varchar(255)\")\n",
    "    db_cursor.execute(\"ALTER TABLE tripad_outlet ADD PRIMARY KEY (id_outlet)\")\n",
    "    db_cursor.execute(\"ALTER TABLE tripad_users MODIFY user varchar(255)\")\n",
    "    db_cursor.execute(\"ALTER TABLE tripad_users ADD PRIMARY KEY (user)\")\n",
    "    db_cursor.execute(\"ALTER TABLE tripad_reviews MODIFY user varchar(255)\")\n",
    "    db_cursor.execute(\"ALTER TABLE tripad_reviews MODIFY id_outlet varchar(255)\")\n",
    "    db_cursor.execute(\"ALTER TABLE tripad_reviews ADD CONSTRAINT PK_reviwes PRIMARY KEY (user,id_outlet)\")\n",
    "    write_msg = ('At {} Tripadvisor Primary keys set  \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "\n",
    "    # Set foregin keys\n",
    "    db_cursor.execute(\"ALTER TABLE tripad_reviews ADD FOREIGN KEY (id_outlet) REFERENCES tripad_outlet(id_outlet)\")\n",
    "    db_cursor.execute(\"ALTER TABLE tripad_reviews ADD FOREIGN KEY (user) REFERENCES tripad_users(user)\")\n",
    "    write_msg = ('At {} Tripadvisor foregin keys set  \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "\n",
    "except:\n",
    "    write_msg = ('At {} Load Tripadvisor table setting relations  has failed \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ubereats database relation setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set primary keys\n",
    "try:\n",
    "    db_cursor.execute(\"ALTER TABLE ubeareats_outlet MODIFY id_outlet varchar(255)\")\n",
    "    db_cursor.execute(\"ALTER TABLE ubeareats_outlet ADD PRIMARY KEY (id_outlet)\")\n",
    "    db_cursor.execute(\"ALTER TABLE ubeareats_menu MODIFY name varchar(255)\")\n",
    "    db_cursor.execute(\"ALTER TABLE ubeareats_menu MODIFY id_outlet varchar(255)\")\n",
    "    db_cursor.execute(\"ALTER TABLE ubeareats_menu MODIFY brand varchar(255)\")\n",
    "    db_cursor.execute(\"ALTER TABLE ubeareats_menu ADD CONSTRAINT PK_reviwes PRIMARY KEY (name,id_outlet,brand)\")\n",
    "    write_msg = ('At {} Ubereats Primary keys set  \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "\n",
    "    #Set foreign keys\n",
    "    db_cursor.execute(\"ALTER TABLE ubeareats_menu ADD FOREIGN KEY (id_outlet) REFERENCES ubeareats_outlet(id_outlet)\")\n",
    "    write_msg = ('At {} Ubereats foregin keys set  \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "except:\n",
    "    write_msg = ('At {} Load Ubereats database relation setting has failed \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small column adjusting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing address formating\n",
    "try:\n",
    "    db_cursor.execute(\"USE dashmotecstudy\")\n",
    "    affected_rows = db_cursor.execute(\"UPDATE dashmotecstudy.tripad_outlet SET address = REPLACE(address,'|',',') , address = REPLACE(address,',,',',')  WHERE id_outlet IS NOT NULL;\")\n",
    "    db_cursor.execute(\"UPDATE dashmotecstudy.ubeareats_outlet SET country = REPLACE(country,'NL','Netherlands') WHERE id_outlet IS NOT NULL;\")\n",
    "    db_connector.commit()\n",
    "    write_msg = ('At {}  table adjusments set \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n",
    "except:\n",
    "    write_msg = ('At {} Load Small column adjusting has failed \\n').format(datetime.now().strftime(\"%m_%d_%Y_%H-%M-%S\"))\n",
    "    log_managment(write_msg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
